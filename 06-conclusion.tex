\section*{Concluding remarks and future directions}
\label{sec:conclusion}

In this article, we reviewed compelling evidence that a variety of neuronal response properties can be understood as an emergent property of efficient population coding based on dimensionality reduction and sparse coding. Remarkably, these neural responses range from sensory systems to memory systems, implying that \ac{NSC} is being applied throughout the brain. We offer three testable predictions of this hypothesis:

First, we predict that parts-based representations can explain
\acp{RF} of neurons in a variety of brain regions,
including but not limited to those brain areas discussed here (i.e., V1, MSTd and RSC). In agreement with the literature on basis function representations
\cite{PougetSejnowski1997,PougetSnyder2000,Poggio1990},
we expect parts-based representations
to be prevalent in regions where neurons
exhibit a range of tuning behaviors \cite{Beyeler2016},
display mixed selectivity \cite{Fusi2016,Eichenbaum2017},
or encode information in multiple reference frames \cite{AlexanderNitz2015,Rounds2016}.

Second, where such representations occur, we expect the resulting
neuronal population activity to be sparse,
in order to encode information both accurately and efficiently.
Sparse codes offer a trade-off between 
dense codes (where every neuron is involved in every context,
leading to great memory capacity but suffering from cross talk among neurons)
and local codes (where there is no interference, 
but also no capacity for generalization) %\cite{Spanne2015417}.

Third, we propose that \ac{STDPH} is carrying out a similar function to \ac{NMF},
and may be attempting to approximate the linear \ac{RF} of
neurons participating in sparse, parts-based representations
throughout the brain. STDPH and NMF can effectively produce NSC.
With the emergence of computational tools developed
to understand the neural code 
in high stimulus dimensions \cite{PillowSimoncelli2006},
we expect to see qualitative similarities between empirically observed
\acp{RF} and those recovered by \ac{NMF} and \ac{STDPH}.
Such findings would be consistent with the idea that neurons
can perform statistical inference on their inputs via
Hebbian-like learning mechanisms
\cite{Nessler2009,Carlson2013,MorenoBoteDrugowitsch2015,Oja1982}.

Furthermore, NSC has implications for computing, artificial intelligence, and machine learning. \ac{NSC} can be used to design neural networks that are highly compressed, so that the number of neurons is far smaller than the number of input features represented. 
Moreover, these networks can represent data with a sparse population code; that is, when input data is presented to the network, only a few neurons are active at any given time, making the code very energy-efficient. At the same time, different data samples might activate a different subset of neurons, making the code highly informative. By taking linear combinations of neural activity (i.e., linear decoding), the system can infer hidden or latent variables that are unobservable in the original high-dimensional dataset \cite{PougetSnyder2000}.
%
% Thoughts/ideas relevant to ML from the Towards the Integration Neuroscience and Deep Learning paper that should be mentioned. Currently rough.
% \mikeNote{Commented out machine learning part}
% \jeffNote{I have some problems with the next two paragraphs.  First, I completely disagree with the idea that backprop is happening in the brain. I think it is the AI field's way of trying to justify biological plausibility of their methods.  So, I don't want to support that argument. Second, I think talking about feedback connections and top-down prediction  confuses things. Up to this point, we have not introduced feedback and we don't have any NSC examples to support that.  Finally, I do find the idea of minimizing a cost function is interesting. Especially, with unsupervised learning. I think that might be similar to what NSC is doing. So, maybe this needs some scaling back and made more specific to our proposal.}

% \textcolor{purple}{\ac{NSC} may support unsupervised learning of cost functions in brain regions. Cost function optimization is a concept widely utilized by machine learning algorithms, and for training neural networks \cite{marblestone2016}. Furthermore, the learned representations that are shaped by local plasticity rules (i.e., STDPH) will be sparse, compact and efficient \cite{olshausen1997,Rozell2008}. Spatiotemporal sparseness in representation also provides an inductive bias that may help accelerate learning \cite{mitchell1980}. These cost functions are further tuned over time by bootstrapping prior knowledge in order to produce more sophisticated representations and complex behaviors, which are further refined through the incorporation of error signals propagated by other connected regions. Thus we suggest that nonnegative sparse coding facilitates efficient learning and cost function optimization, both within and between neural regions.}

% % Scale back backprop, focus on cost function.
% \textcolor{purple}{Unsupervised learning in the form of \ac{NSC} can also support cost function optimization through local self-organization and bootstrapping \cite{marblestone2016}. Synaptic plasticity rules may shape tunable local cost functions independently in disparate brain regions for the purpose of optimizing local function. According to our view, NSC is inherent in the process since the nature of incoming inputs inform the specifications of each local cost function, which for reasons already outlined, likely undergo some form of dimensionality reduction. These local cost functions serve to minimize local prediction error that shape local representations. This view is supported by the observation that minimization of prediction error can explain properties of the nervous system \cite{friston2007,huang2011}, and the process is initiated by exploiting statistical regularities in the environment, consistent with NSC.}

% \textcolor{purple}{Furthermore, the learned representations that result from cost functions shaped by local plasticity rules are sparse since they reflect the sparsity of the environment \cite{olshausen1997,Rozell2008}; for example, the presence of objects in the environment is likely to be sparse. Spatio-temporal sparseness in representation also provides an inductive bias that may help accelerate learning \cite{mitchell1980}. These cost functions are further tuned over time by bootstrapping prior knowledge in order to produce more sophisticated representations and complex behaviors, which are further refined through the incorporation of error signals propagated by other connected regions. Thus we suggest that sparse coding is necessary for efficient learning (cost function optimization).}

% \textcolor{purple}{Cost function optimization is a concept widely utilized by machine learning algorithms, which use supervised learning (generally in the form of backpropagation) to train neural networks \cite{marblestone2016}. Supervised learning depends on the backpropagation of cost function error signals through the network, utilizing a "gradient descent" process. However, unsupervised learning is an important direction for the development of machine learning algorithms if they are to be implemented on neuromorphic hardware \cite{schuman2017}. We suggest that cost function optimization and dimensionality reduction as performed by \ac{NSC} may provide a promising direction for the development of spike-based machine learning algorithms, thereby enabling them to run efficiently under tight power constraints (e.g., on mobile devices).}

% and then some references about how neuromorphic computing primarily uses backprop for training, but that spiking nets/unsupervised learning are the most promising directions
% \mikeNote{Removed ``believe'' with stronger sentiment}
In summary, evidence reviewed in this paper suggests
that dimensionality reduction through nonnegative sparse coding, 
implemented by synaptic plasticity rules, 
might be a canonical computation throughout the brain.
