% THIS SECTION HAS BEEN EXCLUDED FROM THE PAPER



% \label{sec:discussion}
% \subsection{NSC as a ubiquitous coding strategy}
% \ac{NSC} may have emerged as a ubiquitous strategy because reasons
% \subsection{Neurons are nonlinear. Why nonlinear dimensionality reduction?}
% Some researchers say nonlinear reduction techniques do not give you a rich enough representation but we disagree because we did some stuff and it worked and also more reasons.
% \subsection{Where do we go from here?}
% Neuroscience is in a position to actually start looking at RFs in more obscure, hard to reach areas of the brain and we are getting better at looking at the structure of synapses too. Propose new studies.

% % \mikeNote{TODO: better title, maybe use subsection titles instead}


% % % We are currently at 4000 words without the discussion and excluding text boxes/glossary.
% % %...limit is 2500


% % The Discussion section is still a mess.
% % I have compiled some thoughts and notes that I think would be worth incorporating.


% % Let's make sure we touch on the world view (and publications) of the big players in the field: Jonathan Pillow, Surya Ganguli, Maneesh Sahani, Liam Paninski, Mitya Chklovskii(?), Terry Sejnowski(?), Jonathan Victor(?), Wolfgang Maass(?)
% % %  Wolfgang Maass paper on probabilistic inference:
% % % http://eneuro.org/content/eneuro/early/2016/03/25/ENEURO.0048-15.2016.full.pdf
% % % Idea: WTA circuits can pull out underlying statistical structure of training data and can generalize from it
% % %  Seems like Maass might agree with us: "From the perspective of some theories of neural coding and computations it would be desirable that different neurons in a local network can encode independently of each other specific features of a sensory stimulus. However virtually all simultaneous recordings from many neurons within a local patch of cortex suggest that the joint activity patterns of nearby neurons are restricted to variations of a rather small repertoire of spatio-temporal firing patterns."
% % % From  "Searching for principles of brain computation " in current opinion in behavioral sciences


% % % For Pillow, maybe this paper is a good one to talk about...
% % % Has to do with extracting statistics from spike trains, but they argue most dim. red. methods don't capture enough info
% % % https://arxiv.org/pdf/1610.08465.pdf


% % % For Ganguli, we have the compressed sensing paper...

% % %  Seems like Sahanhi emphasizes nonlinearity...
% % % http://www.gatsby.ucl.ac.uk/~maneesh/papers/luecke-sahani-2008-jmlr.pdf


% % %...and if anyone has papers from any of these authors listed that they feel must be mentioned, please list them here.


% % \emilyNote{Kris: mapping synapses that generate activity. Right on the cusp of being able to compare weight matrices deep in the brain, we could propose a study.}




% % In more detail:
% % % From the Ganguli paper
% % The problem of storing, communicating, and processing high-dimensional neural activity
% % patterns, or external stimuli, presents a fundamental challenge to any neural system.
% % This challenge is complicated by the widespread existence of convergent pathways,
% % or bottlenecks, in which information stored in a large number of neurons is often
% % compressed into a small number of axons, or neurons in a downstream system.
% % For example, $1$ million optic nerve fibers carry information about the activity of
% % $100$ times as many photoreceptors.
% % Only $1$ million pyramidal tract fibers carry information from motor cortex to the
% % spinal cord.
% % And corticobasal ganglia pathways undergo a $10$ to $1,000$ fold convergence.

% % Another bottleneck is posed by the task of working memory, where streams of sensory
% % inputs must presumably be stored within the dynamic reverberations of neuronal circuits.
% % This is a bottleneck from time into space: Long temporal streams of input must be stored
% % in the instantaneous spatial activity patterns of a limited number of neurons.



% % \subsection{How could a population of neurons implement dimensionality reduction?}

% % % Interesting note in the Bar-Gad paper: How do neurotransmitters help in shaping the statistical constraints on neural computation?
% % When both input neurons, conveying the feature and its opposite, are connected to the same output neuron using the same neurotransmitter (and therefore the same constraint).  



% % % same here
% % Regularization techniques used by statisticians to learn
% % high-dimensional statistical models from limited amounts of data
% % can also be employed by synaptic learning rules to efficiently search
% % the high-dimensional space of synaptic patterns
% % in order to learn appropriate rules from limited experience.

% % Indeed, it has long been known that neural networks can perform
% % efficient dimensionality reduction using competitive Hebbian learning rules
% % for inter-layer connectivity \citep{Oja1982} and anti-Hebbian rules
% % for the lateral inhibitory intra-layer connectivity
% % \citep{DiamantarasKung1996}.
% % Such a setup leads the network to extract in the weight vector a direction
% % aligned with the first principal component of the input space
% % (in the sense of \ac{PCA}).



% % Also cool to think about 
% % that solving \ac{NMF} is considered to
% % be NP-hard. If \ac{STDPH} can do it, this might be a biological
% % way to solve an NP-hard problem!





% % \subsection{The neuronal code is non-linear. What's with all the talk about \emph{linear} dimensionality reduction?}

% % \mikeNote{Not sure if we need this section - thoughts?}
% % \emilyNote{I think we can sum all this up in a paragraph that might make it into the discussion, but I think a whole section for it is too much.}
% % \mikeNote{Definitely. It was more of an attempt to explain my current line of thinking to the group. :)}
% % Here's one way to think about things:

% % Think of a neuron as this complex, high-dimensional, non-linear machine of which we don't know
% % the input-output function. In fact, it's not feasible to know its input-output function.
% % It's not possible to enumerate all stimuli that would trigger a neuron to respond - this space
% % is simply too large to map out.
% % If you wanted to have a mathematical function that describes a neuron's response to an arbitrary
% % input stimulus, it would have an enormous number of input variables,
% % and the response would be highly non-linear. So this is not feasible.

% % Instead, what we do is we simplify.
% % 1) We might limit our analysis to a small number of input variables, and see how the neuron
% % responds to those.
% % 2) We might use linear techniques to estimate a neuron's response.
% % 3) A combination of 1) and 2).

% % In all cases, we end up with a simplified version of a neuron's response function -
% % or, if you want: a first-order approximation of a neuron's \emph{true} response function.
% % In fact, the traditional ``receptive fields'' of visual neurons are actually retrieved with
% % linear techniques, such as the spike-triggered average:
% % These techniques can only recover the linear relationships of the neuronal response to input
% % data.

% % So when we compare \ac{NMF} representations to classical receptive fields, we are essentially
% % comparing it to these linear receptive field estimations.
% % If we get a good match, it would be foolish to say that a neuron actually implements \ac{NMF},
% % and that \ac{NMF} is all there is to it.
% % If a neuron is a Taylor series, \ac{NMF} is simply a good representation of its first-order term.

% % Don't know if this is useful/relevant:
% % \url{https://papers.nips.cc/paper/5263-low-dimensional-models-of-neural-population-activity-in-sensory-cortical-circuits.pdf}
% % \url{https://pdfs.semanticscholar.org/3ed7/dd40d10cddc1a0715cc0f3493f0be290e34a.pdf}





% % \subsection{How does dimensionality reduction fit with divergent pathways?}

% % % From Ganguli
% % Like convergence, the expansion of neural representations through divergent
% % pathways is a widespread anatomical motif.
% % For example, information in $1$ million optic nerve fibers is expanded into more than
% % $100$ million primary visual cortex neurons.
% % Also, a small number of mossy fibers in the cerebellum target a large number of
% % granule cells, creating a $100$-fold expansion.

% % From Micah's paper: This is
% % interpreted (see e.g. [Olshausen and Field 1996, 1997]) as V1 neurons forming a sparse
% % overcomplete representation of the visual input, with receptive fields of nearby neurons being
% % non-orthogonal and with only a relatively small fraction of neurons being active at any given
% % instant for natural stimuli.
% % \url{http://www.annualreviews.org/doi/pdf/10.1146/annurev-neuro-062111-150410 }

% % Motivated by the observation that in the hippocampus, lots and lots of inputs are coming in through the entorhinal cortex (EC) to the dentate gyrus (DG) where inputs are orthogonalized and made very sparse. 

% % After the DG, inputs are passed into the CA1-CA3 regions of the hippocampus, areas which contain place cells, are thought to hold a representation of an allocentric cognitive map, and also is thought to act as an autoassociative network.

% % The CA1-CA3 regions use the sparse representations from the DG to generate autocompleting representations.

% % Long story short:
% % If you sparsify the representation, you might end up with a representation that requires more
% % neurons than you had to begin with.
% % If you look at the number of neurons, you might prematurely
% % conjecture that the dimensionality went up.
% % But if you look at the embedding, 
% % it might effectively be simpler (lower-dim) 
% % than the input.






% % \subsection{How do we fit with popular theories of brain function?}

% % Data compression shows up in a lot of theories of brain function, but in various forms
% % (e.g., information maximization in efficient coding; free-energy minimization in Friston's stuff;
% % maybe look up Grossberg, Poggio; Heeger put out a new theory, but doesn't touch on compression).
% % More complete theories of brain function usually have other parts, such as prediction and feedback.


% % Would be good to discuss how we relate to Alan Yuille compositional learning stuff.
% % Also: compressed sensing, efficient coding, free energy.

% % Also, \ac{NMF} is related to k-means clustering (check Wikipedia),
% % multinomial PCA (check Wikipedia),
% % and spectral clustering \citep{PehlevanChklovskii2014,Ding2005}.

% % Also, it fits the idea that compression is among the fundamental principles of
% % cortical computation \citep{Richert2016} - could be a nice way
% % of ending the paper: information compression is fundamental!



