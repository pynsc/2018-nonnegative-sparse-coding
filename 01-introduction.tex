% From the Trends guide:
%
% Start with an accessible introduction outlining why the subject is
% important and why you are writing about it now.
% Please use a descriptive heading rather than "Introduction" for this
% section, and be sure to state your opinion clearly.

\section{A need for compressed representations of a high-dimensional world}
\label{sec:introduction}

% \mikeNote{The Sandia footnote now fits on the first page, but I can't seem to get rid of the empty page that follows... I think it's a Latex spacing thing.}
% \emilyNote{Not sure why it caused the blank page, but it looks like the footnote was too long. I worked around it using the geometry package, which allows you to adjust the margins.}
% \mikeNote{Nice job!}
Brains face the fundamental challenge of 
processing, storing, and representing high-dimensional stimuli
using patterns of neural activity.
This challenge is complicated by strict constraints on metabolic cost
\citep{Lennie2003}
and the widespread existence of anatomical bottlenecks 
\citep{Kempermann2002,BarGad2003_Review,Babinsky1993,GanguliSompolinsky2012},
which often force the information stored in a large number of neurons
to be compressed into an orders of magnitude smaller population
of downstream neurons;
for example, storing information from 100 million photoreceptors 
in 1 million optic nerve fibers,
or resulting in a $10 - 10,000$ fold convergence from cortex to the basal ganglia
\citep{GanguliSompolinsky2012}.
% \jeffNote{where did the 10 to 10,000 number come from?  Can you provide a reference?}
% which force the information stored across a large number of neurons 
% to be compressed into a small number of downstream neurons.

One potential approach to addressing this challenge is to reduce the number
of variables required to represent a particular stimulus space;
a process known as \textbf{dimensionality reduction} (see Glossary).
This idea features prominently in 
efficient coding theories of brain function
\citep{Barlow1961,Barlow2001,Atick1992,Linsker1990},
which posit that the brain performs dimensionality reduction 
% Mike: This sentence didn't make sense. The idea is that
% anatomical bottlenecks in the brain (e.g., retina -> V1)
% perform dimensionality reduction. Not "to reduce information
% bottlenecks"
% to reduce information bottlenecks
by maximizing mutual information between the
high-dimensional input and the low-dimensional output 
of neuronal populations.
Several modern variants of the efficient coding hypothesis,
such as \ac{ICA} \citep{BellSejnowski1997}
and \ac{NSC} \citep{Hoyer2002,Hoyer2004}, suggest that the role of cortical representations is to
further reduce the redundancy of the sensory signal 
by separating it into its underlying causes,
a process known as \textbf{factor analysis}.

Here we propose that a variety of neuronal responses
can be understood as an emergent property of efficient population coding
based on dimensionality reduction and sparse coding.
% can be understood by applying sparse coding and dimensionality reduction to a brain region's neuronal inputs.
Specifically, computational evidence from data analyses and computer simulations
suggest that \ac{NSC},
a combination of \ac{NMF} \citep{PaateroTapper1994,LeeSeung1999}
and sparse population coding \citep{Field1994} (Box 1),
generates low-dimensional embeddings of 
high-dimensional stimulus spaces 
that resemble neuronal population responses 
in a variety of brain regions.
Reminiscent of \textbf{basis function} representations
\citep{PougetSejnowski1997,PougetSnyder2000,Poggio1990},
these embeddings are both sparse and parts-based,
allowing for perceptual or behavioral variables of interest
to be represented by taking a linear combination of neuronal responses.
Furthermore, we suggest that these computations might be implemented in the brain
through mechanisms of synaptic plasticity and homeostasis,
which were previously shown to be capable of
performing statistical inference on synaptic inputs
\citep{Nessler2009,Carlson2013,MorenoBoteDrugowitsch2015,Oja1982}.

% The goal of \ac{NSC} is then to find a linear decomposition of \textbf{V}
% that minimizes the reconstruction error,
% while guaranteeing that both \textbf{W} and \textbf{H} are sparse.
% This can be achieved by minimizing the following cost function
% \citep{Hoyer2002}:
% \begin{equation}
% \min_{\mathbf{W}, \mathbf{H}} \frac{1}{2} ||\mathbf{X} -\mathbf{WH}||^2 + \lambda \sum_{ij} f(H_{ij}),
% \end{equation}
% subject to the constraints
% $\forall ij: \mathbf{W}_{ij} \geq 0$, $\mathbf{H}_{ij} \geq 0$, and
% $||w^{(i)}|| = 1$, where $w^{(i)}$ denotes the $i$th column of \textbf{W}.
% Here, the left-hand term describes the reconstruction error, which can
% be minimized with \ac{NMF},
% and the right-hand term describes the sparseness of the decomposition.
% \jeffNote{I don't see anywhere we combine NMF with sparse coding. Or is that sparseness implied?}
% Our experiments were conducted using a built-in function for \ac{NMF} in MATLAB,
% \mikeNote{This sentence will seem out of place to readers who are unaware that you're trying to address one of Jeff's comments. I added a note in the Methods box instead.}
% which incorporates some constant level of sparsity.

Overall, our observations suggest that 
commonly reported neuronal response properties might simply be a 
by-product of neurons performing a biological equivalent of 
dimensionality reduction on their inputs 
via Hebbian-like learning mechanisms.



% Under certain conditions, STDPH might be formally equivalent to \ac{NMF} \citep{Carlson2013, PehlevanChklovskii2014} (Box 2), which decomposes input matrices into key components w (basis functions that are analogous to synaptic weights) and h (a matrix of hidden coefficients by which the basis vectors are weighted). It is known that networks of neurons are able to extract hidden causes in the input signal through STDP \citep{Nessler2009,MorenoBoteDrugowitsch2015}, equivalent to the h part of NMF's method of decomposition.

% \emilyNote{I feel that bringing up STDP's extraction of hidden causes seems out of place without describing how it pertains to dimensionality reduction so I added some stuff about it but it could be worded better/might be too long now.}

% \mikeNote{Therefore, I would just cut it all out... We have the Boxes to explain the math in detail, and we don't have enough space to say things twice.}
