% From the Trends guide:
%
% Start with an accessible introduction outlining why the subject is
% important and why you are writing about it now.
% Please use a descriptive heading rather than "Introduction" for this
% section, and be sure to state your opinion clearly.

\section{A need for compressed representations of a high-dimensional world}
\label{sec:introduction}

% \mikeNote{The Sandia footnote now fits on the first page, but I can't seem to get rid of the empty page that follows... I think it's a Latex spacing thing.}
% \emilyNote{Not sure why it caused the blank page, but it looks like the footnote was too long. I worked around it using the geometry package, which allows you to adjust the margins.}
% \mikeNote{Nice job!}
Brains face the fundamental challenge of 
processing, storing, and representing high-dimensional stimuli
using patterns of neural activity.
This challenge is complicated by strict constraints on metabolic cost
\citep{Lennie2003}
and the widespread existence of anatomical bottlenecks 
\citep{Kempermann2002,BarGad2003_Review,Babinsky1993,GanguliSompolinsky2012},
which often force the information stored in a large number of neurons
to be compressed into an orders of magnitude smaller population
of downstream neurons;
for example, storing information from 100 million photoreceptors 
in 1 million optic nerve fibers,
or resulting in a $10 - 10,000$ fold convergence from cortex to the basal ganglia
\citep{GanguliSompolinsky2012}. \textcolor{red}{Therefore, nervous systems must be tuned to maximize information processing while minimizing energy expenditure.}
% \jeffNote{where did the 10 to 10,000 number come from?  Can you provide a reference?}
% which force the information stored across a large number of neurons 
% to be compressed into a small number of downstream neurons.

One potential approach to addressing this challenge is to reduce the number
of variables required to represent a particular stimulus space;
a process known as \textbf{dimensionality reduction} (see Glossary).
This idea features prominently in 
efficient coding theories of brain function
\citep{Barlow1961,Barlow2001,Atick1992,Linsker1990},
which posit that the brain performs dimensionality reduction 
% Mike: This sentence didn't make sense. The idea is that
% anatomical bottlenecks in the brain (e.g., retina -> V1)
% perform dimensionality reduction. Not "to reduce information
% bottlenecks"
% to reduce information bottlenecks
by maximizing mutual information between the
high-dimensional input and the low-dimensional output 
of neuronal populations.
Several modern variants of the efficient coding hypothesis,
such as \ac{ICA} \citep{BellSejnowski1997}
and \ac{NSC} \citep{Hoyer2002,Hoyer2004}, suggest that the role of cortical representations is to
further reduce the redundancy of the sensory signal 
by separating it into its underlying causes,
a process known as \textbf{factor analysis}.

Here we \textcolor{red}{show} that a variety of neuronal responses
can be understood as an emergent property of efficient population coding
based on dimensionality reduction and sparse coding. \textcolor{red}{Remarkably, responses from sensory systems to memory systems can be explained by \ac{NSC}}.
% can be understood by applying sparse coding and dimensionality reduction to a brain region's neuronal inputs.
\textcolor{red}{Such representations would be advantageous in a metabolically challenged biological system, as well as in an autonomous system operating under real-world constraints.}

Specifically, computational evidence from data analyses and computer simulations suggest that \ac{NSC}, a combination of \ac{NMF} \citep{PaateroTapper1994,LeeSeung1999} and sparse population coding \citep{Field1994} (Box 1), generates low-dimensional embeddings of 
high-dimensional stimulus spaces 
that resemble neuronal population responses 
in a variety of brain regions. Reminiscent of \textbf{basis function} representations
\citep{PougetSejnowski1997,PougetSnyder2000,Poggio1990},
these embeddings are both sparse and parts-based,
allowing for perceptual or behavioral variables of interest
to be represented by taking a linear combination of neuronal responses.
Furthermore, we \textcolor{red}{demonstrate} that these computations \textcolor{red}{can be implemented in the brain
through mechanisms of synaptic plasticity and homeostasis}
\citep{Nessler2009,Carlson2013,MorenoBoteDrugowitsch2015,Oja1982}. \textcolor{red}{Thus, we propose that} 
commonly reported neuronal response properties \textcolor{red}{are a} 
by-product of neurons performing a biological equivalent of 
dimensionality reduction on their inputs 
via Hebbian-like learning mechanisms.
% The goal of \ac{NSC} is then to find a linear decomposition of \textbf{V}
% that minimizes the reconstruction error,
% while guaranteeing that both \textbf{W} and \textbf{H} are sparse.
% This can be achieved by minimizing the following cost function
% \citep{Hoyer2002}:
% \begin{equation}
% \min_{\mathbf{W}, \mathbf{H}} \frac{1}{2} ||\mathbf{X} -\mathbf{WH}||^2 + \lambda \sum_{ij} f(H_{ij}),
% \end{equation}
% subject to the constraints
% $\forall ij: \mathbf{W}_{ij} \geq 0$, $\mathbf{H}_{ij} \geq 0$, and
% $||w^{(i)}|| = 1$, where $w^{(i)}$ denotes the $i$th column of \textbf{W}.
% Here, the left-hand term describes the reconstruction error, which can
% be minimized with \ac{NMF},
% and the right-hand term describes the sparseness of the decomposition.
% \jeffNote{I don't see anywhere we combine NMF with sparse coding. Or is that sparseness implied?}
% Our experiments were conducted using a built-in function for \ac{NMF} in MATLAB,
% \mikeNote{This sentence will seem out of place to readers who are unaware that you're trying to address one of Jeff's comments. I added a note in the Methods box instead.}
% which incorporates some constant level of sparsity.
%
%
\mikeNote{I'm not sure the machine learning paragraph will be relevant to the PLOS Comp Bio audience, so I commented it out for now.}
% \textcolor{red}{These observations are not only important for understanding how the brain encodes information, but they also have implications for machine learning and neuromorphic computing in which sparse coding can result in efficient, low power computations}.
% \textcolor{blue}{In the case of machine learning, dimensionality reduction plays an important role in allowing auto-encoders to create efficient encodings \cite{Bourlard1988} and learn robust feature detectors \cite{Du2017,hinton2006} in an unsupervised setting}. \jeffNote{Need some references for neuromorphic and machine learning} \emilyNote{Added some stuff, may need to be more cohesive} \textcolor{purple}{For example, mapping high-dimensional input data to a low-dimensional space dramatically improves the handling of motion capture data by machine learning algorithms since redundancies in the high-dimensional characteristics of the data are eliminated \cite{zheng2010}. Personal assistant robots that have high-dimensional action spaces may also benefit from using a combination of dimensionality reduction and reinforcement learning to learn appropriate manipulations through reduction of the action space \cite{curran2016}, and dimensionality reduction via PCA has been implemented using memristive arrays though online supervised learning. We expect that memristive arrays could also be used to implement \ac{NSC} through Spike Timing Dependent Plasticity (STDP), which would allow neuromorphic devices to perform dimensionality reduction without sacrificing the energy efficiency of spiking architectures and the flexibility of unsupervised STDP learning rules \cite{choi2017,serrano2013}. Interestingly, there is also some evidence to suggest that the success of deep learning is rooted in the stacking effects of dimensionality reduction in multi-layer auto-encoders \cite{wang2016}.}


% Under certain conditions, STDPH might be formally equivalent to \ac{NMF} \citep{Carlson2013, PehlevanChklovskii2014} (Box 2), which decomposes input matrices into key components w (basis functions that are analogous to synaptic weights) and h (a matrix of hidden coefficients by which the basis vectors are weighted). It is known that networks of neurons are able to extract hidden causes in the input signal through STDP \citep{Nessler2009,MorenoBoteDrugowitsch2015}, equivalent to the h part of NMF's method of decomposition.

% \emilyNote{I feel that bringing up STDP's extraction of hidden causes seems out of place without describing how it pertains to dimensionality reduction so I added some stuff about it but it could be worded better/might be too long now.}

% \mikeNote{Therefore, I would just cut it all out... We have the Boxes to explain the math in detail, and we don't have enough space to say things twice.}
